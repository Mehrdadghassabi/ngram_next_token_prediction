{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTiunDH3gEfs",
        "outputId": "b8e0f090-45f4-4b3e-d1c8-270b140bf13c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-04 20:09:50--  https://raw.githubusercontent.com/Mehrdadghassabi/ngram_next_token_prediction/main/divar_posts_dataset.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30743 (30K) [text/plain]\n",
            "Saving to: ‘divar_posts_dataset.csv.1’\n",
            "\n",
            "\r          divar_pos   0%[                    ]       0  --.-KB/s               \rdivar_posts_dataset 100%[===================>]  30.02K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-08-04 20:09:50 (841 KB/s) - ‘divar_posts_dataset.csv.1’ saved [30743/30743]\n",
            "\n",
            "Requirement already satisfied: hazm in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: fasttext-wheel<0.10.0,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.2)\n",
            "Requirement already satisfied: flashtext<3.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from hazm) (2.7)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (4.3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.8.1)\n",
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.24.3)\n",
            "Requirement already satisfied: python-crfsuite<0.10.0,>=0.9.9 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.10)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.3.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (2.13.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (71.0.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (7.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.66.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.1->hazm) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/Mehrdadghassabi/ngram_next_token_prediction/main/divar_posts_dataset.csv\n",
        "!pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hazm\n",
        "import csv\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import random"
      ],
      "metadata": {
        "id": "iegbxmmXiPXD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('divar_posts_dataset.csv') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    comments = []\n",
        "    for row in reader:\n",
        "        comments.append(row['comment'])\n",
        "print(comments[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcZBTmkEj4Az",
        "outputId": "364ba720-0a49-4446-9e68-b085567f06a2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "سلام,یک عدد گلدون نخل سه طبقه ی سالم دارم با پایه ی سفالی به علت جابجایی میفروشمش\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentences(text):\n",
        "    tokenizer = hazm.SentenceTokenizer()\n",
        "    sentences = tokenizer.tokenize(text)\n",
        "    return sentences\n",
        "\n",
        "comments_sentences = []\n",
        "for comment in comments:\n",
        "    sentences = get_sentences(comment)\n",
        "    comments_sentences.append(sentences)\n",
        "print(comments_sentences[17])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdlB1J9G4Mk_",
        "outputId": "57d01f3b-c904-48da-9d64-67b73b9bbcf7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['تلوزیون رنگی ۲۹ اینچ توشیبا صفحه تخت کاملا سالم همراه با میز فابریک']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):\n",
        "    persian_punctuation = 'ء\\n()/•٪,،؛؟«»٬:.!-…*_×''+$'\n",
        "    cleaned_text = \"\"\n",
        "    for char in text:\n",
        "        if char in persian_punctuation:\n",
        "           cleaned_text = cleaned_text + '\\u200C'\n",
        "        else:\n",
        "           cleaned_text = cleaned_text + char\n",
        "\n",
        "    return cleaned_text"
      ],
      "metadata": {
        "id": "cM2zZYp7iJvB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_persian_numbers(text):\n",
        "    persian_numbers = {\n",
        "        '۰': '0',\n",
        "        '۱': '1',\n",
        "        '۲': '2',\n",
        "        '۳': '3',\n",
        "        '۴': '4',\n",
        "        '۵': '5',\n",
        "        '۶': '6',\n",
        "        '۷': '7',\n",
        "        '۸': '8',\n",
        "        '۹': '9'\n",
        "    }\n",
        "\n",
        "    for persian_num, latin_num in persian_numbers.items():\n",
        "        text = text.replace(persian_num, latin_num)\n",
        "    text = re.sub(r'\\d+', '$Num', text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "cbrjF-Hd3uKM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transliterate(text):\n",
        "    persian_alphabet = {\n",
        "        'a': 'ا',\n",
        "        'b': 'ب',\n",
        "        'c': 'س',\n",
        "        'd': 'د',\n",
        "        'e': '',\n",
        "        'f': 'ف',\n",
        "        'g': 'گ',\n",
        "        'h': 'ه',\n",
        "        'i': 'ی',\n",
        "        'j': 'ج',\n",
        "        'k': 'ک',\n",
        "        'l': 'ل',\n",
        "        'm': 'م',\n",
        "        'n': 'ن',\n",
        "        'o': '',\n",
        "        'p': 'پ',\n",
        "        'q': 'ک',\n",
        "        'r': 'ر',\n",
        "        's': 'س',\n",
        "        't': 'ت',\n",
        "        'u': 'و',\n",
        "        'v': 'و',\n",
        "        'w': 'و',\n",
        "        'x': 'کس',\n",
        "        'y': 'ی',\n",
        "        'z': 'ز'\n",
        "    }\n",
        "\n",
        "    result = ''\n",
        "    for char in text:\n",
        "        if char.lower() in persian_alphabet:\n",
        "            result += persian_alphabet[char.lower()]\n",
        "        else:\n",
        "            result += char\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "sfLThg27LwPq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_comments(comments):\n",
        "    normalizer = hazm.Normalizer()\n",
        "    normalized_comments = []\n",
        "    for comment in comments:\n",
        "        comment = normalizer.normalize(comment)\n",
        "        comment = remove_punctuation(comment)\n",
        "        comment = comment.replace('\\u200C', \" \")\n",
        "        comment = transliterate(comment)\n",
        "        comment = replace_persian_numbers(comment)\n",
        "        comment = comment.replace(\"$Num\", \"\")\n",
        "        comment = comment.replace(\"  \", \" \")\n",
        "        normalized_comments.append(comment)\n",
        "    return normalized_comments"
      ],
      "metadata": {
        "id": "pAopYhOZYeDS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_comments = normalize_comments(comments)\n",
        "print(normalized_comments[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pccjk1_jZxPQ",
        "outputId": "2af581b4-df52-423f-b9f8-a12fc8a9d45d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "کلاسیک و شیک و استثنایی چرم مالزی چوب راش فوق العاده سالم و بدون عیب و ایراد\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ngram_model(comments, n):\n",
        "    ngrams = defaultdict(int)\n",
        "    total_ngrams = 0\n",
        "\n",
        "    for comment in comments:\n",
        "        tokenizer = hazm.WordTokenizer()\n",
        "        words = tokenizer.tokenize(comment)\n",
        "\n",
        "        for i in range(len(words) - n + 1):\n",
        "            ngram = tuple(words[i:i+n])\n",
        "            ngrams[ngram] += 1\n",
        "            total_ngrams += 1\n",
        "\n",
        "    ngram_model = {}\n",
        "    for ngram, count in ngrams.items():\n",
        "        ngram_model[ngram] = count\n",
        "\n",
        "    return ngram_model,total_ngrams"
      ],
      "metadata": {
        "id": "VEIHHJBWX_-z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ngram_count(ngram_model,ngram):\n",
        "    try:\n",
        "      return ngram_model[ngram]\n",
        "    except:\n",
        "      return 0"
      ],
      "metadata": {
        "id": "1Zt9zKbChdl2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_model,total_unigrams = build_ngram_model(comments = normalized_comments, n = 1)\n",
        "unigram_model = dict(sorted(unigram_model.items(), key = lambda x:-x[1]))\n",
        "bigram_model,total_bigrams = build_ngram_model(comments = normalized_comments, n = 2)\n",
        "bigram_model = dict(sorted(bigram_model.items(), key = lambda x:-x[1]))\n",
        "trigram_model,total_trigrams = build_ngram_model(comments = normalized_comments, n = 3)\n",
        "trigram_model = dict(sorted(trigram_model.items(), key = lambda x:-x[1]))"
      ],
      "metadata": {
        "id": "7F8rHbRgajqa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_dic(ngram_model):\n",
        "    dic = {}\n",
        "    for key in ngram_model.keys():\n",
        "        if ngram_model[key] not in dic.keys():\n",
        "           dic[ngram_model[key]] = 1\n",
        "        else:\n",
        "           dic[ngram_model[key]] += 1\n",
        "    return dic"
      ],
      "metadata": {
        "id": "d5I8DIFc0g45"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def N(dic,c):\n",
        "    try:\n",
        "      return dic[c]\n",
        "    except:\n",
        "      return 0.001"
      ],
      "metadata": {
        "id": "CUiUJyN409gu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ngram_probablity1(ngram,word,\n",
        "                     unigram_model,total_unigrams,bigram_model,trigram_model,\n",
        "                     n):\n",
        "    assert n>=1 and n<=3\n",
        "    prob = 0\n",
        "    V = len(unigram_model.keys())\n",
        "    K = 0.005\n",
        "    if n == 1:\n",
        "       prob = (get_ngram_count(unigram_model,(word,)) + K )/(total_unigrams + K*V)\n",
        "    if n == 2:\n",
        "       prob = (get_ngram_count(bigram_model,ngram+(word,)) + K )/(get_ngram_count(unigram_model,ngram) + K*V)\n",
        "    if n == 3:\n",
        "       prob = (get_ngram_count(trigram_model,ngram+(word,)) + K )/(get_ngram_count(bigram_model,ngram) + K*V)\n",
        "\n",
        "    return prob\n"
      ],
      "metadata": {
        "id": "gI4SX7Tvtm9L"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_uniform(dic):\n",
        "    if dic == {} or dic == None:\n",
        "       return choose_uniform(unigram_model)\n",
        "    total = sum(dic.values())\n",
        "    cumulative_sum = 0\n",
        "    rand_num = random.randint(1,total)\n",
        "    for ngram,count in dic.items():\n",
        "        cumulative_sum += count\n",
        "        if rand_num <= cumulative_sum:\n",
        "           return ngram[0]"
      ],
      "metadata": {
        "id": "bydufKpCfWKX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_token(n,previous_words,unigram_model,bigram_model,trigram_model):\n",
        "      if n == 1 :\n",
        "         return choose_uniform(None)\n",
        "      if n == 2 :\n",
        "         di = {}\n",
        "         for ngram,count in bigram_model.items():\n",
        "             if ngram[0] == previous_words[0]:\n",
        "                di[(ngram[1],)] = count\n",
        "         return choose_uniform(di)\n",
        "      if n == 3 :\n",
        "         di = {}\n",
        "         for ngram,count in trigram_model.items():\n",
        "             if ngram[0] == previous_words[0] and ngram[1] == previous_words[1]:\n",
        "                di[(ngram[2],)] = count\n",
        "         return choose_uniform(di)"
      ],
      "metadata": {
        "id": "LIHQd3qQFLuI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "previous_words = None\n",
        "predict_next_token(1,previous_words,unigram_model,bigram_model,trigram_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "k_fWsLuKGLye",
        "outputId": "1ac2394e-dacf-4b41-c018-6cff75bec3fa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'پارچه'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previous_words = ('پیامک',)\n",
        "predict_next_token(2,previous_words,unigram_model,bigram_model,trigram_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FoPsdBnQGXXw",
        "outputId": "92e5750a-3008-490d-bca2-4f15d0ae99ba"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'دهید'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previous_words = ('پیامک','جواب')\n",
        "predict_next_token(3,previous_words,unigram_model,bigram_model,trigram_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SrrSD_NqGgTA",
        "outputId": "21b8ed02-6308-4bf7-e8a3-be5f0e1cf489"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'داده'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def complete_sentence(sentence,n,max_token = 10):\n",
        "    tokenizer = hazm.WordTokenizer()\n",
        "    words = tokenizer.tokenize(sentence)\n",
        "    num_words = len(words)\n",
        "    max_token -= num_words\n",
        "    predicted = sentence\n",
        "    pw = words[len(words)-1]\n",
        "    ppw = words[len(words)-2]\n",
        "    if n == 1:\n",
        "       for i in range(max_token):\n",
        "           w = predict_next_token(n,None,unigram_model,bigram_model,trigram_model)\n",
        "           predicted += ' ' + w\n",
        "    if n == 2:\n",
        "       for i in range(max_token):\n",
        "           previous_words = (pw,)\n",
        "           w = predict_next_token(n,previous_words,unigram_model,bigram_model,trigram_model)\n",
        "           predicted += ' ' + w\n",
        "           pw = w\n",
        "    if n == 3:\n",
        "       for i in range(max_token):\n",
        "           previous_words = (pw,ppw)\n",
        "           w = predict_next_token(n,previous_words,unigram_model,bigram_model,trigram_model)\n",
        "           predicted += ' ' + w\n",
        "           ppw = pw\n",
        "           pw = w\n",
        "    return predicted"
      ],
      "metadata": {
        "id": "OAA0Tx-TIj83"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'مبل هفت نفره خود رنگ'\n",
        "print('unigram prediction: '+str(complete_sentence(sentence,1,max_token = 10)))\n",
        "print('bigram prediction: '+str(complete_sentence(sentence,2,max_token = 10)))\n",
        "print('trigram prediction: '+str(complete_sentence(sentence,3,max_token = 10)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xS4BWUe6O8gj",
        "outputId": "8d23c881-e65a-43ae-d501-dbd772907c51"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unigram prediction: مبل هفت نفره خود رنگ تلویزیون نو همراه بدون تمام\n",
            "bigram prediction: مبل هفت نفره خود رنگ سورمه ای مخصوص فلافل حرفه\n",
            "trigram prediction: مبل هفت نفره خود رنگ نقر محصولات بیمه که مارک\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'دستگاه تردمیل نو'\n",
        "print('unigram prediction: '+str(complete_sentence(sentence,1,max_token = 10)))\n",
        "print('bigram prediction: '+str(complete_sentence(sentence,2,max_token = 10)))\n",
        "print('trigram prediction: '+str(complete_sentence(sentence,3,max_token = 10)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlEMJ3R-PPKr",
        "outputId": "6250c59a-689a-4b45-e954-7312445006b3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unigram prediction: دستگاه تردمیل نو گارانتی تحویل جزئی کارکرد رنگ روزه یا\n",
            "bigram prediction: دستگاه تردمیل نو سالم و سایز نومویک عدد لاستیک پهن\n",
            "trigram prediction: دستگاه تردمیل نو تمیزه داخل عدد گانه دیگه دی هم\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'کفش مردانه'\n",
        "#print('unigram prediction: '+str(complete_sentence(sentence,1,max_token = 10)))\n",
        "print('bigram prediction: '+str(complete_sentence(sentence,2,max_token = 100)))\n",
        "#print('trigram prediction: '+str(complete_sentence(sentence,3,max_token = 10)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0N4L9bAPvvd",
        "outputId": "0af63b26-b8b6-4438-c86c-fb67574b4a8b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigram prediction: کفش مردانه اسپرت بیمه سال تخفیف جزئی داده می شود فقط با جعبه سمس جواب داده نمی کنم سر هم میدم در مصرف انرژی طراحی زیبای مسطح و بدون حتی خط خش جمعا ت و تمیز بدون ظربه خرید و یک عدد تخت بسیار تمیز در ماه گارانتی استفاده شده فنر روغنی تعمیری صندلی اسپرت بند مشکی تمیز و دوسیم کارته صدای قوی رم قدرتمند گیگ حافظه داخلی گیگ دوربین و به دکمه های چرب تو عکس مشخصه چقدر تمیزه گلس درجه یک جفت باند پایونیر و بسیار سالم دارم با قابلیت کنترل کارتن نو با گلدان پایه معامله میدم\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'تعدادی وسایل اداری و پزشکی'\n",
        "#print('unigram prediction: '+str(complete_sentence(sentence,1,max_token = 10)))\n",
        "print('bigram prediction: '+str(complete_sentence(sentence,2,max_token = 100)))\n",
        "#print('trigram prediction: '+str(complete_sentence(sentence,3,max_token = 10)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbvQVLibP8qL",
        "outputId": "34b8a315-408d-4577-c850-d7f06abe1150"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigram prediction: تعدادی وسایل اداری و پزشکی استفاده شده میشه روی جیر تمیز بدون برفک سامانه روشنائی لد طبقه تاشو نگهدارنده بطری تاشو سیستم سرمایش سریع در حد معقول یاحق تلفن هماهنگی نوم نوم تلگرام ممنون از دیوار ایکس باکس وان اکبند اکبند پلمب باز نشده خریدار واقعی تومان کرومیت تومان متری پشم سالم می فروشم تشک نفره جدید با تلگرام و خانه لطفا فقط تماس بگیرید پیامک جواب داده می شود فقط گلگیرها بدون موج رادیو ضبط آیوا نو تاحالا تنش نکرده خودم تا لامپ که بالای قابلمه سفید رینگ اسپرت بیمه تا دارم با تشکر از چوب گردو با زیره\n"
          ]
        }
      ]
    }
  ]
}