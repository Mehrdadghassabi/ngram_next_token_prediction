{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTiunDH3gEfs",
        "outputId": "63026f97-164a-46f3-8375-b78cefe86f48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-29 09:45:43--  https://raw.githubusercontent.com/Mehrdadghassabi/term2_database/main/divar_posts_dataset.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30743 (30K) [text/plain]\n",
            "Saving to: ‘divar_posts_dataset.csv.1’\n",
            "\n",
            "\r          divar_pos   0%[                    ]       0  --.-KB/s               \rdivar_posts_dataset 100%[===================>]  30.02K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-29 09:45:43 (66.6 MB/s) - ‘divar_posts_dataset.csv.1’ saved [30743/30743]\n",
            "\n",
            "Requirement already satisfied: hazm in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: fasttext-wheel<0.10.0,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.2)\n",
            "Requirement already satisfied: flashtext<3.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from hazm) (2.7)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (4.3.2)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.8.1)\n",
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.24.3)\n",
            "Requirement already satisfied: python-crfsuite<0.10.0,>=0.9.9 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.10)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.2.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (2.12.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (67.7.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (6.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.66.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.4.0)\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/Mehrdadghassabi/term2_database/main/divar_posts_dataset.csv\n",
        "!pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hazm\n",
        "import csv\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import random"
      ],
      "metadata": {
        "id": "iegbxmmXiPXD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('divar_posts_dataset.csv') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    comments = []\n",
        "    for row in reader:\n",
        "        comments.append(row['comment'])\n",
        "print(comments[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcZBTmkEj4Az",
        "outputId": "6f9bac86-d471-4dd0-947b-7ecdd5f644e5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "سلام,یک عدد گلدون نخل سه طبقه ی سالم دارم با پایه ی سفالی به علت جابجایی میفروشمش\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentences(text):\n",
        "    tokenizer = hazm.SentenceTokenizer()\n",
        "    sentences = tokenizer.tokenize(text)\n",
        "    return sentences\n",
        "\n",
        "comments_sentences = []\n",
        "for comment in comments:\n",
        "    sentences = get_sentences(comment)\n",
        "    comments_sentences.append(sentences)\n",
        "print(comments_sentences[17])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdlB1J9G4Mk_",
        "outputId": "55ec3ff4-5263-4624-c1f1-260bf30ca2b7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['تلوزیون رنگی ۲۹ اینچ توشیبا صفحه تخت کاملا سالم همراه با میز فابریک']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):\n",
        "    persian_punctuation = 'ء\\n()/•٪,،؛؟«»٬:.!-…*_×''+$'\n",
        "    cleaned_text = \"\"\n",
        "    for char in text:\n",
        "        if char in persian_punctuation:\n",
        "           cleaned_text = cleaned_text + '\\u200C'\n",
        "        else:\n",
        "           cleaned_text = cleaned_text + char\n",
        "\n",
        "    return cleaned_text"
      ],
      "metadata": {
        "id": "cM2zZYp7iJvB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_persian_numbers(text):\n",
        "    persian_numbers = {\n",
        "        '۰': '0',\n",
        "        '۱': '1',\n",
        "        '۲': '2',\n",
        "        '۳': '3',\n",
        "        '۴': '4',\n",
        "        '۵': '5',\n",
        "        '۶': '6',\n",
        "        '۷': '7',\n",
        "        '۸': '8',\n",
        "        '۹': '9'\n",
        "    }\n",
        "\n",
        "    for persian_num, latin_num in persian_numbers.items():\n",
        "        text = text.replace(persian_num, latin_num)\n",
        "    text = re.sub(r'\\d+', '$Num', text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "cbrjF-Hd3uKM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transliterate(text):\n",
        "    persian_alphabet = {\n",
        "        'a': 'ا',\n",
        "        'b': 'ب',\n",
        "        'c': 'س',\n",
        "        'd': 'د',\n",
        "        'e': '',\n",
        "        'f': 'ف',\n",
        "        'g': 'گ',\n",
        "        'h': 'ه',\n",
        "        'i': 'ی',\n",
        "        'j': 'ج',\n",
        "        'k': 'ک',\n",
        "        'l': 'ل',\n",
        "        'm': 'م',\n",
        "        'n': 'ن',\n",
        "        'o': '',\n",
        "        'p': 'پ',\n",
        "        'q': 'ک',\n",
        "        'r': 'ر',\n",
        "        's': 'س',\n",
        "        't': 'ت',\n",
        "        'u': 'و',\n",
        "        'v': 'و',\n",
        "        'w': 'و',\n",
        "        'x': 'کس',\n",
        "        'y': 'ی',\n",
        "        'z': 'ز'\n",
        "    }\n",
        "\n",
        "    result = ''\n",
        "    for char in text:\n",
        "        if char.lower() in persian_alphabet:\n",
        "            result += persian_alphabet[char.lower()]\n",
        "        else:\n",
        "            result += char\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "sfLThg27LwPq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_comments(comments):\n",
        "    normalizer = hazm.Normalizer()\n",
        "    normalized_comments = []\n",
        "    for comment in comments:\n",
        "        comment = normalizer.normalize(comment)\n",
        "        comment = remove_punctuation(comment)\n",
        "        comment = comment.replace('\\u200C', \" \")\n",
        "        comment = transliterate(comment)\n",
        "        comment = replace_persian_numbers(comment)\n",
        "        comment = comment.replace(\"$Num\", \"\")\n",
        "        comment = comment.replace(\"  \", \" \")\n",
        "        normalized_comments.append(comment)\n",
        "    return normalized_comments"
      ],
      "metadata": {
        "id": "pAopYhOZYeDS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_comments = normalize_comments(comments)\n",
        "print(normalized_comments[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pccjk1_jZxPQ",
        "outputId": "cf7ecdf5-645a-47e3-8a1b-d4aa6d15af63"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "کلاسیک و شیک و استثنایی چرم مالزی چوب راش فوق العاده سالم و بدون عیب و ایراد\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ngram_model(comments, n):\n",
        "    ngrams = defaultdict(int)\n",
        "    total_ngrams = 0\n",
        "\n",
        "    for comment in comments:\n",
        "        tokenizer = hazm.WordTokenizer()\n",
        "        words = tokenizer.tokenize(comment)\n",
        "\n",
        "        for i in range(len(words) - n + 1):\n",
        "            ngram = tuple(words[i:i+n])\n",
        "            ngrams[ngram] += 1\n",
        "            total_ngrams += 1\n",
        "\n",
        "    ngram_model = {}\n",
        "    for ngram, count in ngrams.items():\n",
        "        ngram_model[ngram] = count\n",
        "\n",
        "    return ngram_model,total_ngrams"
      ],
      "metadata": {
        "id": "VEIHHJBWX_-z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ngram_count(ngram_model,ngram):\n",
        "    try:\n",
        "      return ngram_model[ngram]\n",
        "    except:\n",
        "      return 0"
      ],
      "metadata": {
        "id": "1Zt9zKbChdl2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_model,total_unigrams = build_ngram_model(comments = normalized_comments, n = 1)\n",
        "unigram_model = dict(sorted(unigram_model.items(), key = lambda x:-x[1]))\n",
        "bigram_model,total_bigrams = build_ngram_model(comments = normalized_comments, n = 2)\n",
        "bigram_model = dict(sorted(bigram_model.items(), key = lambda x:-x[1]))\n",
        "trigram_model,total_trigrams = build_ngram_model(comments = normalized_comments, n = 3)\n",
        "trigram_model = dict(sorted(trigram_model.items(), key = lambda x:-x[1]))"
      ],
      "metadata": {
        "id": "7F8rHbRgajqa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_dic(ngram_model):\n",
        "    dic = {}\n",
        "    for key in ngram_model.keys():\n",
        "        if ngram_model[key] not in dic.keys():\n",
        "           dic[ngram_model[key]] = 1\n",
        "        else:\n",
        "           dic[ngram_model[key]] += 1\n",
        "    return dic"
      ],
      "metadata": {
        "id": "d5I8DIFc0g45"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def N(dic,c):\n",
        "    try:\n",
        "      return dic[c]\n",
        "    except:\n",
        "      return 0.001"
      ],
      "metadata": {
        "id": "CUiUJyN409gu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ngram_probablity1(ngram,word,\n",
        "                     unigram_model,total_unigrams,bigram_model,trigram_model,\n",
        "                     n):\n",
        "    assert n>=1 and n<=3\n",
        "    prob = 0\n",
        "    V = len(unigram_model.keys())\n",
        "    K = 0.005\n",
        "    if n == 1:\n",
        "       prob = (get_ngram_count(unigram_model,(word,)) + K )/(total_unigrams + K*V)\n",
        "    if n == 2:\n",
        "       prob = (get_ngram_count(bigram_model,ngram+(word,)) + K )/(get_ngram_count(unigram_model,ngram) + K*V)\n",
        "    if n == 3:\n",
        "       prob = (get_ngram_count(trigram_model,ngram+(word,)) + K )/(get_ngram_count(bigram_model,ngram) + K*V)\n",
        "\n",
        "    return prob\n"
      ],
      "metadata": {
        "id": "gI4SX7Tvtm9L"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_uniform(dic):\n",
        "    if dic == {} or dic == None:\n",
        "       return choose_uniform(unigram_model)\n",
        "    total = sum(dic.values())\n",
        "    cumulative_sum = 0\n",
        "    rand_num = random.randint(1,total)\n",
        "    for ngram,count in dic.items():\n",
        "        cumulative_sum += count\n",
        "        if rand_num <= cumulative_sum:\n",
        "           return ngram[0]"
      ],
      "metadata": {
        "id": "bydufKpCfWKX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_token(n,previous_words,unigram_model,bigram_model,trigram_model):\n",
        "      if n == 1 :\n",
        "         return choose_uniform(None)\n",
        "      if n == 2 :\n",
        "         di = {}\n",
        "         for ngram,count in bigram_model.items():\n",
        "             if ngram[0] == previous_words[0]:\n",
        "                di[(ngram[1],)] = count\n",
        "         return choose_uniform(di)\n",
        "      if n == 3 :\n",
        "         di = {}\n",
        "         for ngram,count in trigram_model.items():\n",
        "             if ngram[0] == previous_words[0] and ngram[1] == previous_words[1]:\n",
        "                di[(ngram[2],)] = count\n",
        "         return choose_uniform(di)"
      ],
      "metadata": {
        "id": "LIHQd3qQFLuI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "previous_words = None\n",
        "predict_next_token(1,previous_words,unigram_model,bigram_model,trigram_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "k_fWsLuKGLye",
        "outputId": "bbb0fb35-bfe2-4a71-a846-790d912646e8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'کاملا'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previous_words = ('پیامک',)\n",
        "predict_next_token(2,previous_words,unigram_model,bigram_model,trigram_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FoPsdBnQGXXw",
        "outputId": "d81b3cd4-b5ed-4444-fc0c-cfcd256e12e0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'دهید'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previous_words = ('پیامک','جواب')\n",
        "predict_next_token(3,previous_words,unigram_model,bigram_model,trigram_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SrrSD_NqGgTA",
        "outputId": "0850e3a7-c0b9-43ee-b88c-f85a99bca5aa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'داده'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def complete_sentence(sentence,n,max_token = 10):\n",
        "    tokenizer = hazm.WordTokenizer()\n",
        "    words = tokenizer.tokenize(sentence)\n",
        "    num_words = len(words)\n",
        "    max_token -= num_words\n",
        "    predicted = sentence\n",
        "    pw = words[len(words)-1]\n",
        "    ppw = words[len(words)-2]\n",
        "    if n == 1:\n",
        "       for i in range(max_token):\n",
        "           w = predict_next_token(n,None,unigram_model,bigram_model,trigram_model)\n",
        "           predicted += ' ' + w\n",
        "    if n == 2:\n",
        "       for i in range(max_token):\n",
        "           previous_words = (pw,)\n",
        "           w = predict_next_token(n,previous_words,unigram_model,bigram_model,trigram_model)\n",
        "           predicted += ' ' + w\n",
        "           pw = w\n",
        "    if n == 3:\n",
        "       for i in range(max_token):\n",
        "           previous_words = (pw,ppw)\n",
        "           w = predict_next_token(n,previous_words,unigram_model,bigram_model,trigram_model)\n",
        "           predicted += ' ' + w\n",
        "           ppw = pw\n",
        "           pw = w\n",
        "    return predicted"
      ],
      "metadata": {
        "id": "OAA0Tx-TIj83"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'مبل هفت نفره خود رنگ'\n",
        "print('unigram prediction: '+str(complete_sentence(sentence,1,max_token = 10)))\n",
        "print('bigram prediction: '+str(complete_sentence(sentence,2,max_token = 10)))\n",
        "print('trigram prediction: '+str(complete_sentence(sentence,3,max_token = 10)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xS4BWUe6O8gj",
        "outputId": "c8352a29-8d05-498e-c5b6-f20e8d55e6bd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unigram prediction: مبل هفت نفره خود رنگ ومدارک کن مدل پایینتر فروش\n",
            "bigram prediction: مبل هفت نفره خود رنگ کاملا سالم می فرشم و\n",
            "trigram prediction: مبل هفت نفره خود رنگ بگیرید گلدون کنترل جواب گردد\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'دستگاه تردمیل نو'\n",
        "print('unigram prediction: '+str(complete_sentence(sentence,1,max_token = 10)))\n",
        "print('bigram prediction: '+str(complete_sentence(sentence,2,max_token = 10)))\n",
        "print('trigram prediction: '+str(complete_sentence(sentence,3,max_token = 10)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlEMJ3R-PPKr",
        "outputId": "3a89e13a-4e2e-427d-867f-de1c73b384f9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unigram prediction: دستگاه تردمیل نو تمیز درب ک ماشین فرش نو عدد\n",
            "bigram prediction: دستگاه تردمیل نو فروشنده واقعی هم توافقی با چوبکاری پایه\n",
            "trigram prediction: دستگاه تردمیل نو وسایل با شاگرد مختلف تعویض باشه رنگ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'کفش مردانه'\n",
        "#print('unigram prediction: '+str(complete_sentence(sentence,1,max_token = 10)))\n",
        "print('bigram prediction: '+str(complete_sentence(sentence,2,max_token = 100)))\n",
        "#print('trigram prediction: '+str(complete_sentence(sentence,3,max_token = 10)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0N4L9bAPvvd",
        "outputId": "e7b07440-65ac-47db-f8aa-0eaa4a31af0c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigram prediction: کفش مردانه اسپرت بند مشکی بدون خط رنگی بالاش میوفته که توی عکس با قیمت کفش طبی چرم اصل و دوسیم کارته صدای قوی رم خور معاوضه هم معاوضه با آمپ خیلی خوبیه و نو فروشنده واقعی داده نمی کنم میخوام بفروشم به فروش یا برای جهیزیه به شرط نر و تمیز در حدنو و یا خرجی اتاق تعویض با کارتن نو مونده نوش تو عکس فرش را بفرستید فرش متری طرح الیزابت در نمایندگی بازش کردم سایز در بازار و سفارشی با آپاچی مدل بالا تمام عیار برای باربری فروش در مصرف ه با عرض وطولهای مختلف ساده طبقه\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'تعدادی وسایل اداری و پزشکی'\n",
        "#print('unigram prediction: '+str(complete_sentence(sentence,1,max_token = 10)))\n",
        "print('bigram prediction: '+str(complete_sentence(sentence,2,max_token = 100)))\n",
        "#print('trigram prediction: '+str(complete_sentence(sentence,3,max_token = 10)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbvQVLibP8qL",
        "outputId": "71e4e5b4-9843-4768-e8fb-4fcdbb965cb0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigram prediction: تعدادی وسایل اداری و پزشکی استفاده شده فاکتورشم هست و بسته_ای تومن خوردگی لاستیک خوب سه نفره از خریدنش تغییر دکراسیون مبل راحتی شزلون به ابعاد به ذکر است ماشین بوده هندزفری پلمپ قیمت توافقی درضمن اسمش هم توافقی وخوب مردانه اسپورت شیک و نوم نوم نوم نوم تلگرام سپاس کوچیکه وهر ماشین تمیز زیاد میدم تماس تلفنی یا تلگرام پاسخگو هستم و لوکس سال پیامک دهید نهار خوری کریستال ساده طبقه ی کیک بوکسینگ فوری فروشی مناسب جهیزیه به نسل جدید کمپرسورهای کم استفاده نشده مارک ''پیرگاردین'' اصلا نشده فروش سریع در صورت توافقی با یا خرجی اتاق تعویض\n"
          ]
        }
      ]
    }
  ]
}